---
title: "Data Science Test"
author: "Miguel Pereira"
date: "10 April 2020"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Import packages
library(data.table)
library(caret)
library(ggplot2)
library(gridExtra)
library(grid)
library(tidyverse)
library(kableExtra)
library(pROC)
library(magrittr)


#Load dataset
data<-as.data.frame(fread('processed.cleveland.data',na.strings = '?'))

```

## Introduction

This document describes the approach taken to build a model to predict the presence of heart disease. It uses the processed Cleveleand dataset where the outcome is a classification of whether the patient has heart disease (values 1,2,3,4) or not (value 0).

I will approach this problem as a binary classification problem (presence vs. absence of heart disease) and will use the other variables as predictors.

I will take a supervised learning approach to predict heart disease (option 1).

The following document is organised as follows:

1. Descriptive statistics
2. Statistical Modelling
3. Model interpretation and concluding remarks

<br>

## Descriptive statistics

First, the data will be loaded and I will see how the outcome is distributed across the 5 categories.
The dataset has `r nrow(data)` observations (patients) and `r ncol(data)` variables, including the outcome.

Below is a summary of the data a table with the counts of per each outcome class and a table with the counts per class after converting the outcome to a binary variable (0 vs. 1,2,3,4, absense vs. presence of heart disease):

```{r dataset_prep, echo=FALSE, warning=FALSE}
#head(data) #columns have no names

#---Change column names according to the documentation
#Only 14 attributes used:
#1. #3 (age)
#2. #4 (sex)
#3. #9 (cp)
#4. #10 (trestbps)
#5. #12 (chol)
#6. #16 (fbs)
#7. #19 (restecg)
#8. #32 (thalach)
#9. #38 (exang)
#10. #40 (oldpeak)
#11. #41 (slope)
#12. #44 (ca)
#13. #51 (thal)
#14. #58 (num) (the predicted attribute) 

colnames(data)<-c('age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang',
                  'oldpeak','slope','ca','thal','outcome')


#Variable types - converting to the variable types needed for the predictive modelling
numeric.vars<-c('age','trestbps','chol','thalach','oldpeak','slope','ca')
factor.vars<-c('sex','cp','fbs','restecg','exang','thal')
data[,numeric.vars] <- lapply(data[,numeric.vars],as.numeric)
data[,factor.vars] <- lapply(data[,factor.vars],factor)


#Creating a binary outcome variable - yes/no for the presence or absense of heart disease
outcome.bin<-rep('no',times=nrow(data))
outcome.bin[which(data$outcome>0)]<-'yes'

data<-cbind(data,as.factor(outcome.bin))
colnames(data)[15]<-'outcome.bin'


#Tables
#Summary of the data
summary(data) 
#%>%kable("html", row.names = FALSE) %>%kable_styling(position = "left", full_width = FALSE)

#Outcome
table(data$outcome,useNA = 'ifany') %>%
  kable("html", row.names = FALSE, col.names  = c('Class','Freq'),caption = 'Counts per class') %>%
  kable_styling(position = "left", full_width = FALSE)

#Outcome after transformation into binary variable
table(outcome.bin,useNA = 'ifany') %>%
  kable("html", row.names = FALSE, col.names =  c('Class','Freq'),caption = 'Counts per class after converting to binary outcome') %>%
  kable_styling(position = "left", full_width = FALSE)
  


```

<br>

From this data it can be seen that the binary outcome is fairly balanced with `r paste(round(table(data$outcome.bin)[2]/sum(data$outcome)*100,digits=1),'%',sep='')` of patients with heart disease.
The variable ```restecg``` has only 4 subjects with category 1. Given that both categories 1 and 2 mean abnomral changes in the ECG, I will group them in one single category.
Also, the summary of the variables shows that the data is very complete with the variables ```ca``` and ```thal``` having 4 and 2 missing values, respectively. The other variables do not have any missing values.


```{r dataset_prep2, echo=FALSE, warning=FALSE}
#Changing the variable restecg to 2 categories
data$restecg<-factor(ifelse(data$restecg==0,0,1))

```

<br>

### Box plots of continuous variables by presense of heart disease {.tabset}

```{r boxplots_num, echo=FALSE, warnings=FALSE, results='asis'}

for(i in numeric.vars){
    
  cat('\n')
  cat("#### ", i,"\n")

  print(ggplot(data,aes_string(x='outcome.bin',y=paste0(i),col='outcome.bin'))+
    geom_boxplot(show.legend = FALSE)  +
    geom_jitter(width = 0.3, show.legend = FALSE, alpha=0.5) +
    xlab("Heart disease") + ylab(paste0(i)) + ggtitle(paste0(i)))
  
  cat("<br>\n")
  cat('\n')
}



```

<br>

### Bar plots of categorical variables by presense of heart disease {.tabset}

```{r barplots_factor, echo=FALSE, warnings=FALSE, results='asis'}

for(i in factor.vars){
    
  cat('\n')
  cat("#### ", i,"\n")

  print(ggplot(data,aes_string(x=paste0(i),fill='outcome.bin'))+
    geom_bar()  + xlab("Heart disease") + ylab("Count")+
    ggtitle(paste0(i))
    )
  
  cat("<br>\n")
  cat('\n')
}


```


<br>
<br>

## Statistical Modelling

<br>

### Dataset preparation

The goal of this exercise is to create a predictive model of heart disease using the 13 predictors provided. The ideal model will fit the data well while ensuring generalisability to other datasets. With this in mind, I will split the dataset in two parts: a training set (70% of the data) and a test set (30% of the data). The training set will be used to build the model and the test set will be used for external validation. The split will be done such that the proportion of patients with and without heart disease is the same in the two sets.

In addition, the data matrix wil be transformed by creating dummy variables for the factor variables with more than two categories.

The table below shows the number of patients in each category in the train, test and the entire datasets.


```{r train_test_split, echo=FALSE, warnings=FALSE}
#Matrix with dummy variables
preds0<-data %>% select(-outcome)

options(na.action="na.pass")
preds<-model.matrix(outcome.bin~.,preds0)[,-1] #creates the matrix with the predictors and removes the intercept

#Train-test split
set.seed(15)
train.rows<- createDataPartition(y=data$outcome.bin, p=0.7, list=FALSE, times=1)

#Train-test sets
train <- data.frame(preds[train.rows,],outcome.bin=outcome.bin[train.rows])
test <- data.frame(preds[-train.rows,],outcome.bin=outcome.bin[-train.rows])

cbind(table(train$outcome.bin),table(test$outcome.bin),table(data$outcome.bin)) %>%
  kable("html", row.names = FALSE,col.names = c('Training set','Test set','Original data'),caption='Number of patients per outcome category after train-test split') %>%
  kable_styling(position = "left", full_width = FALSE)


```

<br>

### Predictive modelling

For this exercise, I will fit two models to the training set and test their performance in the test set:

1. Elastic net, a linear model that uses regularised regression. The number of patients is larger than the number of predictors but regularisation will be useful to select the important predictors.
2. Support Vector Machines with a radial kernel, a non-linear approach which is useful if there are non-linear relationships between the outcome and predictors.

For both models, accuracy will be used to assess model performance. All the predictors will be standardised and the missing values will be imputed using the k-nearest neighbors (KNN) method.

For hyperparameter tuning and model selection during training, I will use repeated 10-fold cross-validation with 10 repeats. This is something that can be explored in a more in-depth analysis but for conciseness, I will use only these parameters.



```{r elastic_net, echo=FALSE, warnings=FALSE,cache=TRUE}

#Elastic Net model
set.seed(12340)
enet<-train(x=train %>% select(-outcome.bin),
            y=factor(train$outcome.bin),
            method="glmnet",
            metric="Accuracy",
            preProcess = c("center", "scale",'knnImpute'),
            trControl = trainControl(method="repeatedcv", number=10,
                                     repeats=10, savePredictions =
                                       TRUE), tuneLength = 10)


```


```{r svm, echo=FALSE,cache=TRUE}

#SVM model
set.seed(12341)
svm<-train(x=train %>% select(-outcome.bin),
           y=factor(train$outcome.bin),
           method="svmRadial",
           metric="Accuracy",
           preProcess = c("center", "scale",'knnImpute'),
           trControl = trainControl(method="repeatedcv", number=10,
                                    repeats=10, savePredictions =
                                      TRUE,classProbs = T), tuneLength = 10)

```

<br>

### Model performance

In this exercise, accuracy was used as the performance measure for model selection during training. In order to access the performance of the model in the test set, I will also calculate accuracy in the test set and evaluate the AUC.

The following table shows the cross-validation (CV) accuracy, the test accuracy and the AUC for the two models.

```{r perf_table, echo=FALSE,warnings=FALSE, message=FALSE}
#Accuracy function
accuracy=function(preds, ref){
  t=table(preds, ref)
  acc=(t[1,1]+t[2,2])/sum(t)
  return(list("table"=t, "accuracy"=acc))
}

#Predictions on the test set - to get the test set accuracy and AUC
enet.pred<-predict(enet, newdata = test %>% select(-outcome.bin))
svm.pred<-predict(svm, newdata = test %>% select(-outcome.bin))


#Test set accuracy
enet.acc<-round(accuracy(enet.pred,as.factor(test$outcome.bin))$accuracy,2)
svm.acc<-round(accuracy(svm.pred,as.factor(test$outcome.bin))$accuracy,2)


#AUC
enet.pred.prob<-predict(enet, newdata = test %>% select(-outcome.bin),type='prob')
enet.roc<-roc(predictor=enet.pred.prob$yes,response=test$outcome.bin)

svm.pred.prob<-predict(svm, newdata = test %>% select(-outcome.bin),type='prob')
svm.roc<-roc(predictor=svm.pred.prob$yes,response=test$outcome.bin)


#Performance table
perf.table<-data.frame(
  Model=c('Elastic net','SVM - radial kernel'),
  train.acc=c(getTrainPerf(enet)$TrainAccuracy,getTrainPerf(svm)$TrainAccuracy),
  test.acc=c(enet.acc,svm.acc),
  auc=c(enet.roc$auc,svm.roc$auc)
)


perf.table %>% 
  kable("html", row.names = FALSE, col.names=c('Model','Train accuracy','Test accuracy','AUC'), caption = 'Model performence',digits=2) %>%
  kable_styling(position = "left", full_width = FALSE)


```

<br>


### Variable importance

Below are the variable importance plots with the variables ordered from most important to least important in both models. For elastic net, it is also possible to see the variables that were left out.

```{r varImp_plots, echo=FALSE,cache=TRUE,warning=FALSE,message=FALSE}

#Variable importance - Elastic Net and SVM
enet.varImp<-ggplot(data= varImp(enet, scale = FALSE), aes(x=rownames(varImp(enet, scale = FALSE)),y=Overall)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "darkgray") + 
  coord_flip() + geom_point(color='blue') + xlab(" Importance Score")+
  ggtitle("Elastic Net - Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5))

svm.varImp<-ggplot(data= varImp(svm, scale = FALSE), aes(x=rownames(varImp(svm, scale = FALSE)),y=Overall)) +
  geom_bar(position="dodge",stat="identity",width = 0, color = "darkgray") + 
  coord_flip() + geom_point(color='blue') + xlab(" Importance Score")+
  ggtitle("SVM  radial kernel - Variable Importance") + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(enet.varImp, svm.varImp, ncol = 2)

```

<br>

### ROC curves
```{r roc_curves, echo=FALSE,cache=TRUE,warning=FALSE}
#ROC curve template (NOTE: not written by me, obtained on Stack Overflow and I added some changes)
ggroc <- function(roc, showAUC = TRUE, interval = 0.2, breaks = seq(0, 1, interval),
                  title=title){
  require(pROC)
  if(class(roc) != "roc")
    simpleError("Please provide roc object from pROC package")
  plotx <- rev(roc$specificities)
  ploty <- rev(roc$sensitivities)
  
  ggplot(NULL, aes(x = plotx, y = ploty)) +
    geom_segment(aes(x = 0, y = 1, xend = 1,yend = 0), alpha = 0.5) + 
    geom_step() +
    scale_x_reverse(name = "Specificity",limits = c(1,0), breaks = breaks, expand = c(0.001,0.001)) + 
    scale_y_continuous(name = "Sensitivity", limits = c(0,1), breaks = breaks, expand = c(0.001, 0.001)) +
    theme_bw() + 
    theme(axis.ticks = element_line(color = "grey80")) +
    coord_equal() + 
    annotate("text", x = interval/2, y = interval/2, vjust = 0, label = paste("AUC =",sprintf("%.2f",roc$auc))) + ggtitle(title)
}


enet.roc.plot<-ggroc(enet.roc,title='Elastic net')
svm.roc.plot<-ggroc(svm.roc,title='SVM - radial kernel')

grid.arrange(enet.roc.plot,svm.roc.plot,ncol=2)



```

<br>
<br>


## Model interpretation and concluding remarks

The two models show a good performance in predicting the heart disease with elastic net showing slightly better performance in both accuracy and AUC.

The  variable importance plots highlighed the variables that were better preditiors and it is interesting to note that the top 6 most important predictors are the same in both models. This naturally increases confidence in the results.

This was a quick approach to a classification problem. For simplicity, the problem was framed as a binary classification problem but it could be extented to a multi-class classification task. This has the caveat that performance would decrease due to lower numbers per class and due to the number of predictors being quite small to predict 5 classes.

This was a supervised learning approach which could be complemented by using unsupervised learning methods (like PCA, for example) to better understand how the predictors correlate with each other. In this case, PCA would not be particularly useful for dimentionality reduction because the number of predictors is not very large for the number of subjects.



